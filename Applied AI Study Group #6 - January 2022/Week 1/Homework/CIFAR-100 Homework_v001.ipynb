{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check and explore further methods applied the tasks we have previously covered from the following link:\n",
    "https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d313030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton code is provided from the following tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\myEnv001\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda3\\envs\\myEnv001\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\envs\\myEnv001\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions..........\n",
      "torch:        1.10.1+cu102\n",
      "torchvision:  0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Versions..........\")\n",
    "print(\"torch:       \", torch.__version__)\n",
    "print(\"torchvision: \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and testing custom image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DONE] TODO: write at least one data transform or augmentation method yourself ==> DONE\n",
    "def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None, probability=1.0):\n",
    "    \"\"\"\n",
    "    Elastic deformation of images as described in [Simard2003] (with modifications).\n",
    "    [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "    Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "    Proc. of the International Conference on Document Analysis and\n",
    "    Recognition, 2003.\n",
    "\n",
    "    :param image (width,height,channel): An image as a numpy array compatible with cv2.\n",
    "    :param alpha: Size of distortion (pixels). typical value is between 1.5 and 3 x image height\n",
    "    :param sigma: Variance of the Gaussian distribution with whom the uniform random displacement fields will be convolved. Typical value is between 0.01 and 0.1 image height.\n",
    "    :param alpha_affine: Size of the affine transformations. Typical value is between 0.01 and 0.1 image height.\n",
    "    :param random_state: State of the RNG for reproducibility of random displacement fields. Default is None.\n",
    "    :param probability: chance of the transform happening (it is a random deformation). Default is 1.0, meaning the transformation will definitely happen.\n",
    "\n",
    "    :return A new image with the same shape as the input image (numpy array)\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    if np.random.uniform() > probability:\n",
    "        return image\n",
    "    \n",
    "    image = image.numpy() if 'tensor' in type(image).__name__.lower() else image\n",
    "    \n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    \n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
    "\n",
    "\n",
    "def elastic_transform_batch(batch, alpha, sigma, alpha_affine, random_state=None, probability=1.0):\n",
    "    # Perform Elastic Deformation on a whole batch. Here, batch can also be a torch tensor from a dataloader or something.\n",
    "    batch_ = batch.numpy() if 'tensor' in type(batch).__name__.lower() else batch\n",
    "    return np.concatenate([elastic_transform(img, alpha, sigma, alpha_affine, random_state, probability) for img in batch_]).reshape(batch_.shape)\n",
    "\n",
    "\n",
    "# Testing the elastic distorter\n",
    "sampleset = torchvision.datasets.CIFAR100(root='./data/sample', train=False, download=True)\n",
    "sampledata = sampleset.data[np.random.choice(len(sampleset),10),...]\n",
    "#sampledata_deformed = np.concatenate([elastic_transform(img,img.shape[1]*1.1,img.shape[1]*0.08,img.shape[1]*0.08) for img in sampledata]).reshape(10,32,32,3)\n",
    "h,w = sampledata.shape[1:3]\n",
    "sampledata_deformed = elastic_transform_batch(sampledata, w*1.1, w*0.1, w*0.02)\n",
    "plt.figure(figsize = (10,50))\n",
    "i = 0\n",
    "for idx in range(10):\n",
    "    i += 1\n",
    "    plt.subplot(10,2,i)\n",
    "    plt.imshow(sampledata[idx,...])\n",
    "    plt.axis('off')\n",
    "    if i == 1: plt.title('Original')\n",
    "    i += 1\n",
    "    plt.subplot(10,2,i)\n",
    "    plt.imshow(sampledata_deformed[idx,...])\n",
    "    if i == 2: plt.title('Deformed')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we will exercise converting between cv2 images (H,W,C) and torch tensors (C,H,W) or (B,C,H,W)\n",
    "R = 1*np.ones((1080,1920))\n",
    "G = 2*np.ones((1080,1920))\n",
    "B = 3*np.ones((1080,1920))\n",
    "i3 = np.stack([R, G, B], 0)\n",
    "print(\"shape of i3: \", i3.shape)\n",
    "i4 = np.stack([i3,10*i3,100*i3,1000*i3,10000*i3],0)\n",
    "print(\"shape of i4: \", i4.shape)\n",
    "i3c = i4[2,...]\n",
    "print(\"shape of i3c: \",i3c.shape)\n",
    "i3n = i3c.transpose((1,2,0))\n",
    "print(\"shape of i3n: \",i3n.shape)\n",
    "i3n[:,:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# [DONE] TODO: You may consider appyling more transform such as data augmentation methods, etc.\n",
    "# TODO: You may consider hyperparameter optimization: in this cell, we have batch_size!\n",
    "# [DONE] TODO: use the previously defined data transform/augmentation method in the following transform.\n",
    "\n",
    "# Global constants\n",
    "ELASTIC_ALPHA = 32*1.1\n",
    "ELASTIC_SIGMA = 32*0.1\n",
    "ELASTIC_ALPHA_AFFINE = 32*0.02\n",
    "ELASTIC_PROBABILITY = 0.5\n",
    "P_VERT_FLIP = 0.5\n",
    "P_HORZ_FLIP = 0.5\n",
    "JITTER_BRIGHTNESS = 0.1\n",
    "JITTER_SATURATION = 0.1\n",
    "JITTER_CONTRAST = 0.2\n",
    "AFFINE_ROTATION = 50\n",
    "AFFINE_TRANSLATION = (0.1, 0.1)\n",
    "AFFINE_SCALE = (0.5, 1.5)\n",
    "AFFINE_SHEAR = 20\n",
    "NORM_MEAN = (0.5, 0.5, 0.5)\n",
    "NORM_STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'batch_size': 4,\n",
    "    'epochs': 10,\n",
    "    'lr': 0.001,\n",
    "    'num_cnn_blocks': 3,\n",
    "    'num_fc_blocks': 3}\n",
    "\n",
    "# Creating dummy transform\n",
    "class DummyTransform(object):\n",
    "    def __init__(self):\n",
    "        self.globx = 0\n",
    "    def __call__(self, image):\n",
    "        self.globx += 1\n",
    "        #print(\"call %d\"%self.globx)\n",
    "        return image\n",
    "\n",
    "\n",
    "# Generating custom class for elastic deformation\n",
    "class ElasticDeformation(object):\n",
    "    \"\"\"Elastic deformation used as image augmentation for classification task, as per the follwoing paper.\n",
    "    Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\",\n",
    "    in Proc. of the International Conference on Document Analysis and Recognition, 2003.\n",
    "\n",
    "    Note that calling this class's function will treat its image input according to its class.\n",
    "    If the image is a numpy array, it will be treated like an OpenCV image (channels last; H,W,C)\n",
    "    If the image is a torch.tensor, it will be treated accordingly (channels first; C,H,W)\n",
    "\n",
    "    Args:\n",
    "\n",
    "    :param alpha: Size of distortion (pixels). typical value is between 1.5 and 3 x image height\n",
    "    :param sigma: Variance of the Gaussian distribution with whom the uniform random displacement fields will be convolved. Typical value is between 0.01 and 0.1 image height.\n",
    "    :param alpha_affine: Size of the affine transformations. Typical value is between 0.01 and 0.1 image height.\n",
    "    :param random_state: State of the RNG for reproducibility of random displacement fields. Default is None.\n",
    "    :param probability: Chance of the transform happening (it is a random deformation). Default is 1.0, meaning the transformation will definitely happen.\n",
    "    :param output: Type of output for its callable function. Options are 'array' for OpenCV output (channels last), or 'tensor' for PyTorch Tensor (channels first)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, sigma, alpha_affine, random_state=None, probability=1.0, output='array'):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.alpha_affine = alpha_affine\n",
    "        self.random_state = random_state\n",
    "        self.probability = probability\n",
    "        self.output = output\n",
    "\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"Perform elastic transformation on an image using the class parameters for transformation.\n",
    "    \n",
    "        Args:\n",
    "            :param image (numpy.ndarray or torch.tensor): Input image or tensor of image.\n",
    "                Note that calling this class's function will treat its image input according to its class.\n",
    "                If the image is a numpy array, it will be treated like an OpenCV image (channels last; H,W,C)\n",
    "                If the image is a torch.tensor, it will be treated accordingly (channels first; C,H,W)\n",
    "\n",
    "        Returns:\n",
    "            :return image: If output=='array' in constructor, output will be an OpenCV array (channels last), if output=='tensor', a PyTorch tensor (channels first) will be returned.\n",
    "        \"\"\"\n",
    "        if self.random_state is None:\n",
    "            random_state = np.random.RandomState(None)\n",
    "\n",
    "        if np.random.uniform() > self.probability:\n",
    "            if 'tensor' in type(image).__name__.lower():\n",
    "                if self.output=='tensor':\n",
    "                    return image\n",
    "                else:\n",
    "                    return image.numpy().transpose((1,2,0))\n",
    "            else:\n",
    "                if self.output=='tensor':\n",
    "                    return torch.tensor(image.transpose((2,0,1)))\n",
    "                else:\n",
    "                    return image\n",
    "        \n",
    "        image = image.numpy().transpose((1,2,0)) if 'tensor' in type(image).__name__.lower() else image\n",
    "        \n",
    "        shape = image.shape\n",
    "        shape_size = shape[:2]\n",
    "        \n",
    "        # Random affine\n",
    "        center_square = np.float32(shape_size) // 2\n",
    "        square_size = min(shape_size) // 3\n",
    "        pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "        pts2 = pts1 + random_state.uniform(-self.alpha_affine, self.alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), self.sigma) * self.alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), self.sigma) * self.alpha\n",
    "        dz = np.zeros_like(dx)\n",
    "\n",
    "        x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "        indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "        out = map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
    "        return out if self.output=='array' else torch.tensor(out.transpose((2,0,1)))\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+\"(\\n\"+\\\n",
    "            \"    alpha = {},\\n\".format(self.alpha)+\\\n",
    "            \"    sigma = {},\\n\".format(self.sigma)+\\\n",
    "            \"    alpha_affine = {},\\n\".format(self.alpha_affine)+\\\n",
    "            \"    random_state = {},\\n\".format(self.random_state)+\\\n",
    "            \"    probability = {},\\n\".format(self.probability)+\\\n",
    "            \"    output = {})\".format(self.output)\n",
    "\n",
    "\n",
    "\n",
    "def elastic_transform_chosen(img):\n",
    "    return elastic_transform(img, ELASTIC_ALPHA, ELASTIC_SIGMA, ELASTIC_ALPHA_AFFINE, random_state=None, probability=ELASTIC_PROBABILITY)\n",
    "\n",
    "# Image augmentation transforms\n",
    "transform2 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomVerticalFlip(P_VERT_FLIP),\n",
    "        transforms.RandomHorizontalFlip(P_HORZ_FLIP),\n",
    "        transforms.ColorJitter(brightness=JITTER_BRIGHTNESS, contrast=JITTER_CONTRAST, saturation=JITTER_SATURATION),\n",
    "        transforms.RandomAffine(AFFINE_ROTATION, AFFINE_TRANSLATION, AFFINE_SCALE, AFFINE_SHEAR),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        DummyTransform(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomVerticalFlip(P_VERT_FLIP),\n",
    "        transforms.RandomHorizontalFlip(P_HORZ_FLIP),\n",
    "        transforms.ColorJitter(brightness=JITTER_BRIGHTNESS, contrast=JITTER_CONTRAST, saturation=JITTER_SATURATION),\n",
    "        transforms.RandomAffine(AFFINE_ROTATION, AFFINE_TRANSLATION, AFFINE_SCALE, AFFINE_SHEAR),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ElasticDeformation(ELASTIC_ALPHA, ELASTIC_SIGMA, ELASTIC_ALPHA_AFFINE, random_state=None, probability=ELASTIC_PROBABILITY, output='tensor'),\n",
    "        transforms.RandomVerticalFlip(P_VERT_FLIP),\n",
    "        transforms.RandomHorizontalFlip(P_HORZ_FLIP),\n",
    "        transforms.ColorJitter(brightness=JITTER_BRIGHTNESS, contrast=JITTER_CONTRAST, saturation=JITTER_SATURATION),\n",
    "        transforms.RandomAffine(AFFINE_ROTATION, AFFINE_TRANSLATION, AFFINE_SCALE, AFFINE_SHEAR),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data/CIFAR100', train=True, download=True, transform=transform1)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data/CIFAR100', train=False, download=True, transform=transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=hparams['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('apples', 'aquarium fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottles', \n",
    "           'bowls', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'cans', 'castle', 'caterpillar', 'cattle', 'chair', \n",
    "           'chimpanzee', 'clock', 'cloud', 'cockroach', 'computer keyboard', 'couch', 'crab', 'crocodile', \n",
    "           'cups', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', \n",
    "           'kangaroo', 'lamp', 'lawn-mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple', 'motorcycle', \n",
    "           'mountain', 'mouse', 'mushrooms', 'oak', 'oranges', 'orchids', 'otter', 'palm', 'pears', 'pickup truck', \n",
    "           'pine', 'plain', 'plates', 'poppies', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', \n",
    "           'roses', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', \n",
    "           'streetcar', 'sunflowers', 'sweet peppers', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor',\n",
    "           'train', 'trout', 'tulips', 'turtle', 'wardrobe', 'whale', 'willow', 'wolf', 'woman', 'worm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing some normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get some random training images\n",
    "dataiter = iter(testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = dataiter.next()\n",
    "#images, labels = testloader[0]\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(labels)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(hparams['batch_size'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base method we provide:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 200)\n",
    "        self.fc2 = nn.Linear(200, 128)\n",
    "        self.fc3 = nn.Linear(128, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional but highly recommended) TODO: You can define your own neural network to create a better performing model!\n",
    "class MyOwnNet(Net):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train on GPU:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not forget to initialize your own network if you defined one!\n",
    "#inzvaNet = MyOwnNet()\n",
    "inzvaNet = Net()\n",
    "\n",
    "inzvaNet.to(device)\n",
    "\n",
    "inzvaNet.train()\n",
    "# base optimizer with following parameters:\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(inzvaNet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# TODO: play with hyperparameters and chosen methods to achieve higher accuracy! You can apply grid or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = inzvaNet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "outputs = inzvaNet(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on all test data\n",
    "inzvaNet.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = inzvaNet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct)\n",
    "print(total)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise accuracy\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = inzvaNet(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
