{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK6xwObDkjDs"
   },
   "source": [
    "# Anomaly Detection Homework\n",
    "\n",
    "This notebook is for anomaly detection homework of Applied AI Week 4. The dataset is given with [this link](https://drive.google.com/file/d/1cZGOZu_zdKLXnH-Ap1w9SMffYXZqa2Ot/view?usp=sharing). If you are having problems with the link, contact with me: safak@inzva.com\n",
    "\n",
    "## Dataset Description\n",
    "\"KDD CUP 99 data set is used mainly to analyze the different\n",
    "attacks. It consists of nearly 4,900,000 samples with 41\n",
    "features and each sample is classified as either normal or\n",
    "attack\" [explanation from this source](https://www.ripublication.com/ijaer18/ijaerv13n7_81.pdf)\n",
    "\n",
    "## Task Description\n",
    "\n",
    "The dataset is prepared and preprocessed for anomaly detection task, the dataset contains \"Probe\" and \"Normal\" targets. \"Probe\" is anomaly, \"Normal\" is normal. \n",
    "\n",
    "**You are supposed to build a anomaly detection model** with **Vanilla Autoencoder**, **Variational Autoencoder** and **Denoising Autoencoder**. However you are not restricted by autoencoer, you can implement a fancy state-of-the-art ensemble 1000B parameter model. It is really up to you. \n",
    "\n",
    "We don't really want you to do sloppy homework.\n",
    "\n",
    "The variable descriptions:\n",
    "\n",
    "- train set: kdd_train_probe\n",
    "- validation set (for hyperparam tuning): kdd_valid_probe\n",
    "- test set: kdd_test_v2_probe\n",
    "\n",
    "## What will you report?\n",
    "Report your average macro f1 score on test set:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "print(f1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIZsN_bUmDaD"
   },
   "source": [
    "# Preparation (do not edit this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "0foq7wEgm3hD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BHWVxbClBdhl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IjeIew14vgeQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "kdd = pd.read_csv('kdd.csv')\n",
    "kdd = kdd.iloc[:,1:43]\n",
    "kdd = kdd.drop(['Protocol Type', 'Service', 'Flag'], axis = 1)\n",
    "\n",
    "kdd_train = kdd.iloc[0:102563, :]\n",
    "kdd_test = kdd.iloc[102563:183737, :]\n",
    "\n",
    "kdd_train_probe = kdd_train[(kdd_train.Type_Groups == 'Normal') | (kdd_train.Type_Groups == 'Probe')]\n",
    "kdd_test_probe = kdd_test[(kdd_test.Type_Groups == 'Normal') | (kdd_test.Type_Groups == 'Probe')]\n",
    "\n",
    "kdd_train_probe['Type_Groups'] = np.where(kdd_train_probe['Type_Groups'] == 'Normal', 0, 1)\n",
    "kdd_test_probe['Type_Groups'] = np.where(kdd_test_probe['Type_Groups'] == 'Normal', 0, 1)\n",
    "\n",
    "kdd_valid_probe = kdd_test_probe.iloc[14000:34000,:]\n",
    "kdd_test_v2_probe = pd.concat([kdd_test_probe.iloc[0:14000,:], kdd_test_probe.iloc[34001:64759,:]])\n",
    "\n",
    "\n",
    "# classify anomalies and normals\n",
    "# train set: kdd_train_probe\n",
    "# validation set (for hyperparam tuning): kdd_valid_probe\n",
    "# test set: kdd_test_v2_probe\n",
    "# avg. macro f1 score on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcxDLPSSmIPd"
   },
   "source": [
    "## Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O7bI7a2t2J4R"
   },
   "outputs": [],
   "source": [
    "# NORMAL: class label 0\n",
    "# ANOMALY: class label 1\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(TabularDataset, self).__init__()\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx, :-1].to_numpy()\n",
    "        return {\n",
    "            \"samples\": torch.Tensor(data)\n",
    "        }\n",
    "    \n",
    "class TabularDatasetTest(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(TabularDatasetTest, self).__init__()\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx, :-1].to_numpy()\n",
    "        label = self.df.iloc[idx, -1]\n",
    "        return {\n",
    "            \"samples\": torch.Tensor(data),\n",
    "            \"labels\": torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_normal = kdd_train_probe[kdd_train_probe.Type_Groups == 0]\n",
    "val_normal = kdd_valid_probe[kdd_valid_probe.Type_Groups == 0]\n",
    "test_normal = kdd_test_v2_probe[kdd_test_v2_probe.Type_Groups == 0]\n",
    "\n",
    "\n",
    "train_data = TabularDataset(train_normal)\n",
    "val_data = TabularDataset(val_normal)\n",
    "test_data_all = TabularDatasetTest(kdd_test_v2_probe)\n",
    "\n",
    "# train_dataloader: For training autoencoder. Contains only normal samples\n",
    "# val_dataloader: For evaluating autoencoder at training phase.\n",
    "#                 then use it for tune the threshold value.\n",
    "#                 N.B: setting batch size of 1 at threshold finding phase should be more reasonable:\n",
    "#                 DataLoader(val_data, shuffle = False, batch_size = 1)\n",
    "#\n",
    "# test_all_dataloader: Contains all test samples (anomalies and normals). Use it for\n",
    "#                      calculating your metrics\n",
    "\n",
    "# N.B.: finding a threshold value is challenging. iterating all val_dataloader and calculating\n",
    "#       metrics over it works but it is expensive computationally.\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_data, shuffle = False, batch_size = BATCH_SIZE)\n",
    "test_all_dataloader = DataLoader(test_data_all, shuffle = False, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPMxDRei_bpI"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6h9mMOzXwVdR"
   },
   "outputs": [],
   "source": [
    "# VAE implementation in PyTorch\n",
    "\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim):\n",
    "        super(LinearVAE, self).__init__()\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 20),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.encoder2mean = nn.Linear(20, latent_dim)\n",
    "        self.encoder2logvar = nn.Linear(20, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, n_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        out = self.encoder(x)\n",
    "        mu = self.encoder2mean(out)\n",
    "        log_var = self.encoder2logvar(out)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mu + (eps * std)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if GPU is avvailable for us\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  38\n"
     ]
    }
   ],
   "source": [
    "# Calculating number of features\n",
    "num_features = next(iter(train_dataloader))[\"samples\"].shape[-1]\n",
    "print(\"number of features: \", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=38, out_features=20, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (encoder2mean): Linear(in_features=20, out_features=5, bias=True)\n",
       "  (encoder2logvar): Linear(in_features=20, out_features=5, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating VAE\n",
    "vae = LinearVAE(num_features, latent_dim=5)\n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "Efh0YtnP1zSJ"
   },
   "outputs": [],
   "source": [
    "def vaeloss(out, mu, logvar, target):\n",
    "    # reconstruction loss\n",
    "    mse_loss = F.mse_loss(out, target, reduction=\"mean\")\n",
    "    # KL Divergence loss\n",
    "    # kl = - 0.5 * torch.sum(1 + logvar - torch.square(mu) - torch.square(torch.exp(logvar))) # Most probably incorrect\n",
    "    kl = - 0.5 * torch.sum(1 + logvar - torch.square(mu) - torch.exp(logvar))\n",
    "    # Total loss\n",
    "    loss = mse_loss + kl\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, validation_dataloader, criterion=None):\n",
    "    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n",
    "    num_validation_batches = len(validation_dataloader)\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dictionary in validation_dataloader:\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            out, mu, sigma = model(data)\n",
    "            if criterion:\n",
    "                loss += criterion(out, data)\n",
    "            else:\n",
    "                loss += vaeloss(out, mu, sigma, data)\n",
    "    return loss/num_validation_batches\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, training_dataloader, validation_dataloader, validation_loss_tolerance, num_epochs=5):\n",
    "    # implement training function over ```train_dataloader``` variable\n",
    "    # do not forget KL divergence :)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_training_batches = len(training_dataloader)\n",
    "    num_validation_batches = len(validation_dataloader)\n",
    "    progress_bar_size = 20.0\n",
    "\n",
    "    ch = \"█\"\n",
    "    intvl = num_training_batches/progress_bar_size;\n",
    "    valtol = validation_loss_tolerance\n",
    "    minvalerr = 1000000000.0\n",
    "    badvalcount = 0\n",
    "\n",
    "    tStart = timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        tEpochStart = timer()\n",
    "        epoch_loss_training = 0.0\n",
    "        epoch_loss_validation = 0.0\n",
    "        newnum = 0\n",
    "        oldnum = 0\n",
    "\n",
    "        print(\"Epoch %3d/%3d [\"%(epoch+1, num_epochs), end=\"\")\n",
    "        model.train()\n",
    "        for i, dictionary in enumerate(training_dataloader):\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions, mu, logvar = model(data)\n",
    "            loss = vaeloss(predictions, mu, logvar, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_training += loss.item()\n",
    "            # Visualization of progressbar\n",
    "            newnum = int(i/intvl)\n",
    "            if newnum > oldnum:\n",
    "                print((newnum-oldnum)*ch, end=\"\")\n",
    "                oldnum = newnum\n",
    "        print(\"] \", end=\"\")\n",
    "        epoch_loss_training /= num_training_batches\n",
    "\n",
    "        # epoch_loss_validation = evaluate(model, validation_dataloader, criterion=criterion)\n",
    "        # epoch_loss_validation = evaluate(model, validation_dataloader, criterion=None)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dictionary in validation_dataloader:\n",
    "                data = dictionary[\"samples\"].to(device)\n",
    "                predictions, mu, logvar = model(data)\n",
    "                loss = vaeloss(predictions, mu, logvar, data)\n",
    "                epoch_loss_validation += loss.item()\n",
    "        epoch_loss_validation /= num_validation_batches\n",
    "\n",
    "\n",
    "\n",
    "        tEpochEnd = timer()\n",
    "        print(\"Trn Loss: %5.3f |Val Loss: %5.3f |Time: %6.3f sec\" % (\n",
    "            epoch_loss_training, \n",
    "            epoch_loss_validation, tEpochEnd-tEpochStart))\n",
    "        \n",
    "        # Checking for early stopping\n",
    "        if epoch_loss_validation < minvalerr:\n",
    "            minvalerr = epoch_loss_validation\n",
    "            badvalcount = 0\n",
    "        else:\n",
    "            badvalcount += 1\n",
    "            if badvalcount > valtol:\n",
    "                print(\"Validation loss not improved for more than %d epochs.\"%badvalcount)\n",
    "                print(\"Early stopping criterion with validation loss has been reached. Stopping training at %d epochs...\"%epoch)\n",
    "                break\n",
    "\n",
    "    tFinish = timer()        \n",
    "    print('Finished Training.')\n",
    "    print(\"Training process took %.2f seconds.\"%(tFinish-tStart))\n",
    "    print(\"Saving model...\")\n",
    "    try:\n",
    "        torch.save(model, \"vae_model.pt\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Failed to save the model.\")\n",
    "    print(\"Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200 [███████████████████] Trn Loss: 1.269 |Val Loss: 0.685 |Time: 26.219 sec\n",
      "Epoch   2/200 [███████████████████] Trn Loss: 0.586 |Val Loss: 0.654 |Time: 26.093 sec\n",
      "Epoch   3/200 [███████████████████] Trn Loss: 0.576 |Val Loss: 0.646 |Time: 26.198 sec\n",
      "Epoch   4/200 [███████████████████] Trn Loss: 0.572 |Val Loss: 0.639 |Time: 26.164 sec\n",
      "Epoch   5/200 [███████████████████] Trn Loss: 0.571 |Val Loss: 0.635 |Time: 26.082 sec\n",
      "Epoch   6/200 [███████████████████] Trn Loss: 0.569 |Val Loss: 0.633 |Time: 26.140 sec\n",
      "Epoch   7/200 [███████████████████] Trn Loss: 0.569 |Val Loss: 0.632 |Time: 26.110 sec\n",
      "Epoch   8/200 [███████████████████] Trn Loss: 0.569 |Val Loss: 0.631 |Time: 26.100 sec\n",
      "Epoch   9/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.629 |Time: 26.014 sec\n",
      "Epoch  10/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.628 |Time: 26.175 sec\n",
      "Epoch  11/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.627 |Time: 26.045 sec\n",
      "Epoch  12/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.627 |Time: 26.190 sec\n",
      "Epoch  13/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.626 |Time: 26.117 sec\n",
      "Epoch  14/200 [███████████████████] Trn Loss: 0.568 |Val Loss: 0.626 |Time: 25.760 sec\n",
      "Epoch  15/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.614 sec\n",
      "Epoch  16/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.812 sec\n",
      "Epoch  17/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.936 sec\n",
      "Epoch  18/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.560 sec\n",
      "Epoch  19/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.671 sec\n",
      "Epoch  20/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.600 sec\n",
      "Epoch  21/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.627 |Time: 25.550 sec\n",
      "Epoch  22/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.640 sec\n",
      "Epoch  23/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.709 sec\n",
      "Epoch  24/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.545 sec\n",
      "Epoch  25/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.751 sec\n",
      "Epoch  26/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.700 sec\n",
      "Epoch  27/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.530 sec\n",
      "Epoch  28/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.361 sec\n",
      "Epoch  29/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.625 |Time: 25.412 sec\n",
      "Epoch  30/200 [███████████████████] Trn Loss: 0.567 |Val Loss: 0.624 |Time: 25.332 sec\n",
      "Validation loss not improved for more than 11 epochs.\n",
      "Early stopping criterion with validation loss has been reached. Stopping training at 29 epochs...\n",
      "Finished Training.\n",
      "Training process took 775.13 seconds.\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "vae = train(vae, train_dataloader, val_dataloader, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_f1_score(model, testing_dataloader, threshold):\n",
    "    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n",
    "    # by using predefined threshold value\n",
    "    # if overall loss > threshold, then it is anomaly; else normal\n",
    "    tot_loss = 0\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for dictionary in testing_dataloader:\n",
    "            data = dictionary[\"samples\"]\n",
    "            label = dictionary[\"labels\"]\n",
    "            out, _, _ = model(data)\n",
    "            # reconstruction loss\n",
    "            loss = mean_squared_error(data, out)\n",
    "            pred = 1 if loss > threshold else 0\n",
    "            preds.append(pred)\n",
    "            trues.append(label[0])\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    f1 = f1_score(trues,preds)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7886486486486486\n"
     ]
    }
   ],
   "source": [
    "vae_f1 = calculate_f1_score(vae, test_all_dataloader, 1)\n",
    "print(vae_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLgaxnaR_eEp"
   },
   "source": [
    "# Vanilla AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "9gwWry8U-3Zs"
   },
   "outputs": [],
   "source": [
    "class VanillaAE(nn.Module):\n",
    "    # implement vanilla autoencoder in PyTorch\n",
    "    def __init__(self, sizevec):\n",
    "        super(VanillaAE, self).__init__()\n",
    "        assert(sizevec[0]==sizevec[-1])\n",
    "        self.layers = []\n",
    "        self.sizevec = sizevec\n",
    "        old = sizevec[0]\n",
    "        new = sizevec[1]\n",
    "        for i,size in enumerate(sizevec[1:]):\n",
    "            self.layers.append(nn.Linear(old, new))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            old = new\n",
    "            if i < len(sizevec)-2:\n",
    "                new = sizevec[i+2]\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaAE(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=38, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=10, out_features=38, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiation\n",
    "sizevec = [num_features, 10, 5, 10, num_features]\n",
    "vanae = VanillaAE(sizevec)\n",
    "vanae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for dictionary in dataloader:\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            out = model(data)\n",
    "            tot_loss += criterion(data, out)\n",
    "    tot_loss /= num_batches\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_dataloader, validation_dataloader, validation_loss_tolerance, num_epochs):\n",
    "    # implement training function over ```train_dataloader``` variable\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_training_batches = len(training_dataloader)\n",
    "    num_validation_batches = len(validation_dataloader)\n",
    "    progress_bar_size = 20.0\n",
    "\n",
    "    ch = \"█\"\n",
    "    intvl = num_training_batches/progress_bar_size;\n",
    "    valtol = validation_loss_tolerance\n",
    "    minvalerr = 1000000000.0\n",
    "    badvalcount = 0\n",
    "\n",
    "    tStart = timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        tEpochStart = timer()\n",
    "        epoch_loss_training = 0.0\n",
    "        epoch_loss_validation = 0.0\n",
    "        newnum = 0\n",
    "        oldnum = 0\n",
    "\n",
    "        print(\"Epoch %3d/%3d [\"%(epoch+1, num_epochs), end=\"\")\n",
    "        model.train()\n",
    "        for i, dictionary in enumerate(training_dataloader):\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(data)\n",
    "            loss = criterion(data, predictions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_training += loss.item()\n",
    "            # Visualization of progressbar\n",
    "            newnum = int(i/intvl)\n",
    "            if newnum > oldnum:\n",
    "                print((newnum-oldnum)*ch, end=\"\")\n",
    "                oldnum = newnum\n",
    "        print(\"] \", end=\"\")\n",
    "        epoch_loss_training /= num_training_batches\n",
    "\n",
    "        epoch_loss_validation = evaluate(model, validation_dataloader, criterion)\n",
    "\n",
    "        tEpochEnd = timer()\n",
    "        print(\"Trn Loss: %5.3f |Val Loss: %5.3f |Time: %6.3f sec\" % (\n",
    "            epoch_loss_training, \n",
    "            epoch_loss_validation, tEpochEnd-tEpochStart))\n",
    "        \n",
    "        # Checking for early stopping\n",
    "        if epoch_loss_validation < minvalerr:\n",
    "            minvalerr = epoch_loss_validation\n",
    "            badvalcount = 0\n",
    "        else:\n",
    "            badvalcount += 1\n",
    "            if badvalcount > valtol:\n",
    "                print(\"Validation loss not improved for more than %d epochs.\"%badvalcount)\n",
    "                print(\"Early stopping criterion with validation loss has been reached. Stopping training at %d epochs...\"%epoch)\n",
    "                break\n",
    "\n",
    "    tFinish = timer()        \n",
    "    print('Finished Training.')\n",
    "    print(\"Training process took %.2f seconds.\"%(tFinish-tStart))\n",
    "    print(\"Saving model...\")\n",
    "    try:\n",
    "        torch.save(model, \"vanae_model.pt\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Failed to save the model.\")\n",
    "    print(\"Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200 [███████████████████] Trn Loss: 0.488 |Val Loss: 0.505 |Time: 25.055 sec\n",
      "Epoch   2/200 [███████████████████] Trn Loss: 0.397 |Val Loss: 0.468 |Time: 25.049 sec\n",
      "Epoch   3/200 [███████████████████] Trn Loss: 0.362 |Val Loss: 0.410 |Time: 25.538 sec\n",
      "Epoch   4/200 [███████████████████] Trn Loss: 0.338 |Val Loss: 0.358 |Time: 24.951 sec\n",
      "Epoch   5/200 [███████████████████] Trn Loss: 0.313 |Val Loss: 0.325 |Time: 25.019 sec\n",
      "Epoch   6/200 [███████████████████] Trn Loss: 0.298 |Val Loss: 0.270 |Time: 24.982 sec\n",
      "Epoch   7/200 [███████████████████] Trn Loss: 0.285 |Val Loss: 0.246 |Time: 24.973 sec\n",
      "Epoch   8/200 [███████████████████] Trn Loss: 0.279 |Val Loss: 0.242 |Time: 25.032 sec\n",
      "Epoch   9/200 [███████████████████] Trn Loss: 0.278 |Val Loss: 0.237 |Time: 25.140 sec\n",
      "Epoch  10/200 [███████████████████] Trn Loss: 0.276 |Val Loss: 0.244 |Time: 25.134 sec\n",
      "Epoch  11/200 [███████████████████] Trn Loss: 0.274 |Val Loss: 0.241 |Time: 25.076 sec\n",
      "Epoch  12/200 [███████████████████] Trn Loss: 0.275 |Val Loss: 0.241 |Time: 25.006 sec\n",
      "Epoch  13/200 [███████████████████] Trn Loss: 0.274 |Val Loss: 0.245 |Time: 25.017 sec\n",
      "Epoch  14/200 [███████████████████] Trn Loss: 0.274 |Val Loss: 0.238 |Time: 24.983 sec\n",
      "Epoch  15/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.231 |Time: 24.958 sec\n",
      "Epoch  16/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.232 |Time: 24.793 sec\n",
      "Epoch  17/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.237 |Time: 24.811 sec\n",
      "Epoch  18/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.231 |Time: 24.802 sec\n",
      "Epoch  19/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.231 |Time: 24.800 sec\n",
      "Epoch  20/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.231 |Time: 24.847 sec\n",
      "Epoch  21/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.230 |Time: 24.540 sec\n",
      "Epoch  22/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.228 |Time: 24.444 sec\n",
      "Epoch  23/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.232 |Time: 24.376 sec\n",
      "Epoch  24/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.228 |Time: 24.474 sec\n",
      "Epoch  25/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.226 |Time: 24.400 sec\n",
      "Epoch  26/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.238 |Time: 24.371 sec\n",
      "Epoch  27/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.225 |Time: 26.123 sec\n",
      "Epoch  28/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.226 |Time: 24.486 sec\n",
      "Epoch  29/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.226 |Time: 24.401 sec\n",
      "Epoch  30/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.231 |Time: 24.485 sec\n",
      "Epoch  31/200 [███████████████████] Trn Loss: 0.272 |Val Loss: 0.227 |Time: 24.391 sec\n",
      "Epoch  32/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.226 |Time: 24.383 sec\n",
      "Epoch  33/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.227 |Time: 24.409 sec\n",
      "Epoch  34/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.228 |Time: 24.532 sec\n",
      "Epoch  35/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.234 |Time: 24.766 sec\n",
      "Epoch  36/200 [███████████████████] Trn Loss: 0.271 |Val Loss: 0.242 |Time: 24.535 sec\n",
      "Epoch  37/200 [███████████████████] Trn Loss: 0.273 |Val Loss: 0.234 |Time: 24.452 sec\n",
      "Epoch  38/200 [███████████████████] Trn Loss: 0.270 |Val Loss: 0.239 |Time: 24.319 sec\n",
      "Validation loss not improved for more than 11 epochs.\n",
      "Early stopping criterion with validation loss has been reached. Stopping training at 37 epochs...\n",
      "Finished Training.\n",
      "Training process took 941.89 seconds.\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "vanae = train(vanae, train_dataloader, val_dataloader, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(model, testing_dataloader, threshold):\n",
    "    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n",
    "    # by using predefined threshold value\n",
    "    # if overall loss > threshold, then it is anomaly; else normal\n",
    "    tot_loss = 0\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for dictionary in testing_dataloader:\n",
    "            data = dictionary[\"samples\"]\n",
    "            label = dictionary[\"labels\"]\n",
    "            out = model(data)\n",
    "            # reconstruction loss\n",
    "            loss = mean_squared_error(data, out)\n",
    "            pred = 1 if loss > threshold else 0\n",
    "            preds.append(pred)\n",
    "            trues.append(label[0])\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    f1 = f1_score(trues,preds)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8965271593944791\n"
     ]
    }
   ],
   "source": [
    "vanae_f1 = calculate_f1_score(vanae, test_all_dataloader, 1.5)\n",
    "print(vanae_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0v3fAUHHlde"
   },
   "source": [
    "# DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_Iyj39x6HmJq"
   },
   "outputs": [],
   "source": [
    "class DenoisingAE(nn.Module):\n",
    "    # implement denoising autoencoder in PyTorch\n",
    "    def __init__(self, sizevec):\n",
    "        super(DenoisingAE, self).__init__()\n",
    "        assert(sizevec[0]==sizevec[-1])\n",
    "        self.layers = []\n",
    "        self.sizevec = sizevec\n",
    "        old = sizevec[0]\n",
    "        new = sizevec[1]\n",
    "        for i,size in enumerate(sizevec[1:]):\n",
    "            self.layers.append(nn.Linear(old, new))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            old = new\n",
    "            if i < len(sizevec)-2:\n",
    "                new = sizevec[i+2]\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenoisingAE(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=38, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=10, out_features=38, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiation\n",
    "sizevec = [num_features, 10, 5, 10, num_features]\n",
    "dae = DenoisingAE(sizevec)\n",
    "dae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Btk1uZLvIOam"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, noisiness):\n",
    "    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for dictionary in dataloader:\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            data += torch.randn_like(data)*noisiness\n",
    "            out = model(data)\n",
    "            tot_loss += criterion(data, out)\n",
    "    tot_loss /= num_batches\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_dataloader, validation_dataloader, validation_loss_tolerance, num_epochs, noisiness):\n",
    "    # implement training function over ```train_dataloader``` variable\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_training_batches = len(training_dataloader)\n",
    "    num_validation_batches = len(validation_dataloader)\n",
    "    progress_bar_size = 20.0\n",
    "\n",
    "    ch = \"█\"\n",
    "    intvl = num_training_batches/progress_bar_size;\n",
    "    valtol = validation_loss_tolerance\n",
    "    minvalerr = 1000000000.0\n",
    "    badvalcount = 0\n",
    "\n",
    "    tStart = timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        tEpochStart = timer()\n",
    "        epoch_loss_training = 0.0\n",
    "        epoch_loss_validation = 0.0\n",
    "        newnum = 0\n",
    "        oldnum = 0\n",
    "\n",
    "        print(\"Epoch %3d/%3d [\"%(epoch+1, num_epochs), end=\"\")\n",
    "        model.train()\n",
    "        for i, dictionary in enumerate(training_dataloader):\n",
    "            data = dictionary[\"samples\"].to(device)\n",
    "            data += torch.randn_like(data)*noisiness\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(data)\n",
    "            loss = criterion(data, predictions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_training += loss.item()\n",
    "            # Visualization of progressbar\n",
    "            newnum = int(i/intvl)\n",
    "            if newnum > oldnum:\n",
    "                print((newnum-oldnum)*ch, end=\"\")\n",
    "                oldnum = newnum\n",
    "        print(\"] \", end=\"\")\n",
    "        epoch_loss_training /= num_training_batches\n",
    "\n",
    "        epoch_loss_validation = evaluate(model, validation_dataloader, criterion, noisiness)\n",
    "\n",
    "        tEpochEnd = timer()\n",
    "        print(\"Trn Loss: %5.3f |Val Loss: %5.3f |Time: %6.3f sec\" % (\n",
    "            epoch_loss_training, \n",
    "            epoch_loss_validation, tEpochEnd-tEpochStart))\n",
    "        \n",
    "        # Checking for early stopping\n",
    "        if epoch_loss_validation < minvalerr:\n",
    "            minvalerr = epoch_loss_validation\n",
    "            badvalcount = 0\n",
    "        else:\n",
    "            badvalcount += 1\n",
    "            if badvalcount > valtol:\n",
    "                print(\"Validation loss not improved for more than %d epochs.\"%badvalcount)\n",
    "                print(\"Early stopping criterion with validation loss has been reached. Stopping training at %d epochs...\"%epoch)\n",
    "                break\n",
    "\n",
    "    tFinish = timer()        \n",
    "    print('Finished Training.')\n",
    "    print(\"Training process took %.2f seconds.\"%(tFinish-tStart))\n",
    "    print(\"Saving model...\")\n",
    "    try:\n",
    "        torch.save(model, \"dae_model.pt\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Failed to save the model.\")\n",
    "    print(\"Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200 [███████████████████] Trn Loss: 0.412 |Val Loss: 0.563 |Time: 25.907 sec\n",
      "Epoch   2/200 [███████████████████] Trn Loss: 0.393 |Val Loss: 0.539 |Time: 26.119 sec\n",
      "Epoch   3/200 [███████████████████] Trn Loss: 0.374 |Val Loss: 0.510 |Time: 25.306 sec\n",
      "Epoch   4/200 [███████████████████] Trn Loss: 0.358 |Val Loss: 0.500 |Time: 25.320 sec\n",
      "Epoch   5/200 [███████████████████] Trn Loss: 0.355 |Val Loss: 0.496 |Time: 25.225 sec\n",
      "Epoch   6/200 [███████████████████] Trn Loss: 0.352 |Val Loss: 0.494 |Time: 25.099 sec\n",
      "Epoch   7/200 [███████████████████] Trn Loss: 0.351 |Val Loss: 0.492 |Time: 25.041 sec\n",
      "Epoch   8/200 [███████████████████] Trn Loss: 0.352 |Val Loss: 0.491 |Time: 24.965 sec\n",
      "Epoch   9/200 [███████████████████] Trn Loss: 0.350 |Val Loss: 0.493 |Time: 25.033 sec\n",
      "Epoch  10/200 [███████████████████] Trn Loss: 0.348 |Val Loss: 0.492 |Time: 25.092 sec\n",
      "Epoch  11/200 [███████████████████] Trn Loss: 0.343 |Val Loss: 0.489 |Time: 25.051 sec\n",
      "Epoch  12/200 [███████████████████] Trn Loss: 0.342 |Val Loss: 0.490 |Time: 25.087 sec\n",
      "Epoch  13/200 [███████████████████] Trn Loss: 0.342 |Val Loss: 0.493 |Time: 25.106 sec\n",
      "Epoch  14/200 [███████████████████] Trn Loss: 0.342 |Val Loss: 0.492 |Time: 25.189 sec\n",
      "Epoch  15/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.489 |Time: 25.073 sec\n",
      "Epoch  16/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.489 |Time: 25.181 sec\n",
      "Epoch  17/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.488 |Time: 25.073 sec\n",
      "Epoch  18/200 [███████████████████] Trn Loss: 0.342 |Val Loss: 0.490 |Time: 24.868 sec\n",
      "Epoch  19/200 [███████████████████] Trn Loss: 0.342 |Val Loss: 0.489 |Time: 24.827 sec\n",
      "Epoch  20/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.489 |Time: 24.821 sec\n",
      "Epoch  21/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.488 |Time: 24.823 sec\n",
      "Epoch  22/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.490 |Time: 24.803 sec\n",
      "Epoch  23/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.491 |Time: 24.815 sec\n",
      "Epoch  24/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.489 |Time: 24.839 sec\n",
      "Epoch  25/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.488 |Time: 25.444 sec\n",
      "Epoch  26/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.488 |Time: 25.177 sec\n",
      "Epoch  27/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.489 |Time: 24.804 sec\n",
      "Epoch  28/200 [███████████████████] Trn Loss: 0.341 |Val Loss: 0.488 |Time: 24.855 sec\n",
      "Validation loss not improved for more than 11 epochs.\n",
      "Early stopping criterion with validation loss has been reached. Stopping training at 27 epochs...\n",
      "Finished Training.\n",
      "Training process took 702.97 seconds.\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "dae = train(dae, train_dataloader, val_dataloader, 10, 200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(model, testing_dataloader, threshold):\n",
    "    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n",
    "    # by using predefined threshold value\n",
    "    # if overall loss > threshold, then it is anomaly; else normal\n",
    "    tot_loss = 0\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for dictionary in testing_dataloader:\n",
    "            data = dictionary[\"samples\"]\n",
    "            label = dictionary[\"labels\"]\n",
    "            out = model(data)\n",
    "            # reconstruction loss\n",
    "            loss = mean_squared_error(data, out)\n",
    "            pred = 1 if loss > threshold else 0\n",
    "            preds.append(pred)\n",
    "            trues.append(label[0])\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    f1 = f1_score(trues,preds)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8188768009408998\n"
     ]
    }
   ],
   "source": [
    "dae_f1 = calculate_f1_score(dae, test_all_dataloader, 1.5)\n",
    "print(dae_f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Homework 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
